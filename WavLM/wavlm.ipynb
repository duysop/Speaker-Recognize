{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "fe = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"C:/Users/duyma/Documents/GitHub/Speaker-Recognize/WavLM/Wav30fr\", num_labels=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "path = 'H:/train_data/F7_40.wav'\n",
    "audio, sample_rate = sf.read(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n",
      "tensor([[9.9907e-05, 1.0670e-06, 6.5833e-05, 6.8741e-05, 6.3354e-04, 8.7489e-05,\n",
      "         2.7999e-04, 6.4739e-05, 1.0865e-03, 1.5499e-06, 8.8968e-04, 1.9577e-04,\n",
      "         9.9447e-01, 1.1267e-03, 2.3199e-04, 1.0921e-04, 1.2259e-05, 9.2181e-05,\n",
      "         4.7413e-05, 8.0309e-05, 1.0277e-06, 1.6266e-05, 5.9120e-07, 9.7765e-05,\n",
      "         5.4640e-05, 8.6921e-07, 8.0118e-05, 8.9838e-05, 1.2384e-05]])\n",
      "F7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duyma\\miniconda3\\envs\\CVenv\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "encodeing = fe(audio, sampling_rate= sample_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "  outputs = model(**encodeing)\n",
    "  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "a=np.argmax(predictions.numpy())\n",
    "pred=np.array([a])\n",
    "print(pred)\n",
    "print(predictions)\n",
    "\n",
    "with open('C:/Users/duyma/Documents/GitHub/Speaker-Recognize/WavLM/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "# Decode the encoded data\n",
    "decoded_value = label_encoder.inverse_transform(pred)[0]\n",
    "print(decoded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_file, extract_to):\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "zip_file = 'C:/Users/duyma/Documents/GitHub/Speaker-Recognize/WavLM/Wav30fr.zip'  # Path to your ZIP file\n",
    "extract_to = 'C:/Users/duyma/Documents/GitHub/Speaker-Recognize/WavLM/Wav30fr'  # Directory where you want to extract the files\n",
    "\n",
    "unzip_file(zip_file, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
